import torch
import torch.nn as nn

class TorchGraph(nn.Module):
    '''Torch图'''
    def __init__(self):
        super(TorchGraph, self).__init__()
        # 创建一个图字典
        self._graph = {}

    def add_tensor_list(self, name):
        # 向图字典里追加张量列表
        self._graph[name] = []

    def append_tensor(self, name, val):
        # 向图字典的张量列表里添加张量
        self._graph[name].append(val)

    def clear_tensor_list(self, name):
        # 清空张量列表
        self._graph[name].clear()

    def get_tensor_list(self, name):
        # 获得张量列表
        return self._graph[name]

    def set_global_var(self, name, val):
        # 设置全局变量
        self._graph[name] = val

    def get_global_var(self, name):
        # 获得全局变量
        return self._graph[name]


class HardSigmoid(nn.Module):
    '''hard sigmoid-->实际上是 2*sigmoid-1 '''
    def __init__(self, inplace=True):
        super(HardSigmoid, self).__init__()
        self.relu = nn.ReLU6(inplace)

    def forward(self, x):
        return self.relu(x+3)/6

class DynamicReLU(nn.Module):
    '''动态ReLU'''
    def __init__(self, inplanes, outplanes, K=2, reduction=8):
        super(DynamicReLU, self).__init__()
        self.inplanes = inplanes
        self.outplanes = outplanes
        self.K = K
        self.reduction = reduction
        self.middle_planes = max(inplanes // reduction, 2)

        # channel attention
        # 全局平均池化(将特征图大小弄成1x1)
        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))
        # fc1
        self.fc1 = nn.Linear(inplanes, self.middle_planes)
        # fc2
        self.fc2 = nn.Linear(self.middle_planes, 2*K*outplanes)
        # relu
        self.relu = nn.ReLU(inplace=True)
        # hard sigmoid
        self.sigmoid = HardSigmoid(inplace=True)

        # 超参数a
        self.alpha = torch.zeros(K, 1)  # 初始化
        self.alpha[0][0] = 1.0  # 将第一个Kernel设为1
        self.alpha = torch.nn.Parameter(self.alpha)  # 将超参数a参数化
        self.alpha.requires_grad = False  # 不使用梯度下降训练超参数a

        # 超参数b
        self.beta = torch.nn.Parameter(torch.zeros(K, 1))  # 初始化b并且参数化
        self.beta.requires_grad = False  # 不适用梯度下降训练超参数b

        # 控制超参数a和b的scaler
        self.lambda_a = 1.0
        self.lambda_b = 0.5

    def forward(self, input):
        # batch_size
        batch_size = input.size(0)
        # GAP
        x = self.global_pool(input)
        # Flatten
        x = x.view(batch_size, -1)
        # fc1
        x = self.fc1(x)
        # relu
        x = self.relu(x)
        # fc2
        x = self.fc2(x)
        # 2*sigmoid-1
        x = 2 * self.sigmoid(x) - 1.0
        # Flatten(n, K, C)-->n=2意思是a或者b
        x = x.view(2, self.K, -1)
        # delta_a和delta_b
        delta_a = x[0]
        delta_b = x[1]
        # 更新a和b
        a = self.alpha.detach() + self.lambda_a * delta_a
        b = self.beta.detach() + self.lambda_b * delta_b
        # Flatten(Kernel, m, C, H, W)
        a = a.view(self.K, batch_size, self.outplanes, 1, 1)
        b = b.view(self.K, batch_size, self.outplanes, 1, 1)

        return a, b


class DynamicReLUV1(nn.Module):
    '''动态ReLUV1'''
    def __init__(self, inplanes, outplanes, K=2, reduction=8):
        super(DynamicReLUV1, self).__init__()
        # inplanes
        self.inplanes = inplanes
        # outplanes
        self.outplanes = outplanes
        # Kernel
        self.K = K
        # reduction
        self.reduction = reduction
        # middle_planes
        self.middle_planes = max(inplanes // reduction, 2)

        # channel attention
        # GAP
        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))
        # fc
        self.fc = nn.Linear(inplanes, K*outplanes)
        # relu
        self.relu = nn.ReLU(inplace=True)
        # sigmoid
        self.sigmoid = HardSigmoid(inplace=True)

        # alpha
        self.alpha = torch.zeros(K, 1)
        self.alpha[0][0] = 1.0
        self.alpha = torch.nn.Parameter(self.alpha)
        self.alpha.requires_grad = False
        self.lambda_a = 1.0

        # beta
        self.b = torch.nn.Parameter(torch.zeros(K*outplanes))

    def forward(self, input):
        # batch_size
        batch_size = input.size(0)
        # GAP
        x = self.global_pool(input)
        # Flatten
        x = x.view(batch_size, -1)
        # fc
        x = self.fc(x)
        # sigmoid
        x = 2*self.sigmoid(x) - 1.0

        # delta_a(K, -1)
        delta_a = x.view(self.K, -1)

        # 更新a和b
        a = self.alpha.detach() + self.lambda_a * delta_a
        b = self.b.repeat(batch_size, 1)

        # Flatten(K, m, C, H, W)
        a = a.view(self.K, batch_size, self.outplanes, 1, 1)
        b = b.view(self.K, batch_size, self.outplanes, 1, 1)

        return a, b


class DynamicGatedReLU(nn.Module):
    '''动态门ReLU'''
    def __init__(self, inplanes, outplanes, ratio, K=2, reduction=8):
        super(DynamicGatedReLU, self).__init__()
        self.inplanes = inplanes
        self.outplanes = outplanes
        self.ratio = ratio
        self.K = K
        self.reduction = reduction

        self.dyrelu = DynamicReLU(inplanes, outplanes, K, reduction)

    def forward(self, input1, input2):
        # 获得超参数a和b
        a, b = self.dyrelu(input1)
        # 得到不激活的通道数
        inactive_channels = self.outplanes - round(self.outplanes * self.ratio)
        # 留下值较大的那些Kernel，返回的是(value, indices)
        gates, _ = torch.max(a, dim=0)
        # 对于不同的输入样本找到c个值较大的通道，返回的是(value, indices)，这里我们要的是indices
        inactive_idx = (-gates).topk(inactive_channels, dim=1)[1]
        # 添加第一维度K
        inactive_idx = inactive_idx.unsqueeze(0)
        # 在第一维度K复制一次
        inactive_idx = inactive_idx.repeat(2, 1, 1, 1, 1)
        # 对通道维度数据按照inactive_idx重分配
        a = a.scatter(2, inactive_idx, 0)  # scatter(dim, index, src) dim->沿着哪个维度索引  index->索引  src->用来 scatter 的源元素
        b = b.scatter(2, inactive_idx, 0)  # scatter(dim, index, src) dim->沿着哪个维度索引  index->索引  src->用来 scatter 的源元素

        # 动态ReLU函数的表达式：max(ax+b)
        x = a * input2 + b
        x, _ = torch.max(x, dim=0)

        return x, gates
